{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NBA Basketball Foul Detection - Colab Training\n",
    "\n",
    "This notebook trains the E2E-Spot model for basketball foul detection on Google Colab Pro.\n",
    "\n",
    "## Setup Overview\n",
    "\n",
    "1. **Environment Setup** - Install dependencies (~2 min)\n",
    "2. **Mount Google Drive** - Connect persistent storage (~10 sec)\n",
    "3. **AWS Credentials** - Configure S3 access (~30 sec)\n",
    "4. **Download Frames to Drive** - One-time download of 22GB (~10-15 min)\n",
    "5. **Clone Repository** - Get training code (~30 sec)\n",
    "6. **Verify Setup** - Check everything is ready (~10 sec)\n",
    "7. **Start Training** - Begin training (~14-17 hours with Colab Pro)\n",
    "8. **Resume Training** - Continue if session disconnects\n",
    "\n",
    "## Important Notes\n",
    "\n",
    "- **Colab Pro:** 24-hour sessions (vs 12 hours free)\n",
    "- **Frames:** Downloaded once to Drive, persistent across sessions\n",
    "- **Checkpoints:** Saved directly to Drive, no separate backup needed\n",
    "- **Total time:** ~14-17 hours (might finish in one Colab Pro session)\n",
    "\n",
    "**Enable GPU:** Runtime \u2192 Change runtime type \u2192 GPU (T4/V100/A100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Environment Setup\n",
    "\n",
    "Install PyTorch and dependencies. Run this first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "print(\"Installing dependencies for Colab environment...\")\n",
    "!pip install -q torch torchvision timm tqdm tabulate opencv-python pillow matplotlib\n",
    "\n",
    "# Verify GPU availability\n",
    "import torch\n",
    "print(f\"\\n\u2713 PyTorch version: {torch.__version__}\")\n",
    "print(f\"\u2713 CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\u2713 GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"\u2713 GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    # Colab Pro may give you V100 (32GB) or A100 (40GB)\n",
    "else:\n",
    "    print(\"\\n\u26a0\ufe0f  WARNING: No GPU detected!\")\n",
    "    print(\"Please enable GPU: Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(\"\\n\u2713 Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Mount Google Drive\n",
    "\n",
    "Connect Google Drive for persistent storage of frames and checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create directories in Drive (persistent across sessions)\n",
    "DRIVE_ROOT = '/content/drive/MyDrive/nba_foul_training'\n",
    "FRAME_DIR = f'{DRIVE_ROOT}/frames'\n",
    "CHECKPOINT_DIR = f'{DRIVE_ROOT}/checkpoints'\n",
    "\n",
    "os.makedirs(DRIVE_ROOT, exist_ok=True)\n",
    "os.makedirs(FRAME_DIR, exist_ok=True)\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"\\n\u2713 Google Drive mounted successfully\")\n",
    "print(f\"\u2713 Frames will be stored in: {FRAME_DIR}\")\n",
    "print(f\"\u2713 Checkpoints will be saved to: {CHECKPOINT_DIR}\")\n",
    "print(f\"\\nCurrent Drive contents:\")\n",
    "!ls -lh \"$DRIVE_ROOT\" 2>/dev/null || echo \"  (empty)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: AWS Credentials\n",
    "\n",
    "Configure AWS credentials to download frames from S3.\n",
    "\n",
    "**Security:** Credentials are stored only in this session (temporary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "print(\"Enter your AWS credentials (input is hidden):\")\n",
    "print(\"These are used ONLY in this Colab session to download frames from S3.\")\n",
    "print(\"Your local AWS config is NOT affected.\\n\")\n",
    "\n",
    "AWS_ACCESS_KEY_ID = getpass(\"AWS Access Key ID: \")\n",
    "AWS_SECRET_ACCESS_KEY = getpass(\"AWS Secret Access Key: \")\n",
    "AWS_REGION = input(\"AWS Region (default: us-east-2): \") or \"us-east-2\"\n",
    "\n",
    "# Set environment variables (temporary, session-only)\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = AWS_ACCESS_KEY_ID\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = AWS_SECRET_ACCESS_KEY\n",
    "os.environ['AWS_DEFAULT_REGION'] = AWS_REGION\n",
    "\n",
    "# Install AWS CLI\n",
    "print(\"\\nInstalling AWS CLI...\")\n",
    "!pip install -q awscli\n",
    "\n",
    "# Test credentials (without displaying them)\n",
    "import subprocess\n",
    "result = subprocess.run(['aws', 's3', 'ls', 's3://nba-foul-dataset-oh/'], \n",
    "                       capture_output=True, text=True)\n",
    "if result.returncode == 0:\n",
    "    print(\"\\n\u2713 AWS credentials verified successfully!\")\n",
    "    print(\"\u2713 S3 bucket access confirmed\")\n",
    "else:\n",
    "    print(\"\\n\u274c AWS credential verification failed. Please check your keys.\")\n",
    "    print(f\"Error: {result.stderr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Download Frames to Google Drive\n",
    "\n",
    "**Downloads 22GB of frame data from S3 directly to Google Drive.**\n",
    "\n",
    "- **First time:** Takes ~10-15 minutes\n",
    "- **Subsequent sessions:** Skipped (frames already in Drive)\n",
    "- **Persistent:** Frames stay in Drive across sessions\n",
    "\n",
    "\u26a0\ufe0f **This is the critical step.** Make sure it completes successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport re\nimport time\n\nS3_BUCKET = 's3://nba-foul-dataset-oh/frames/'\n\ndef count_clips_in_dir(base_dir):\n    \"\"\"Count unique clips from both foul and non-foul directories\"\"\"\n    clips = set()\n    \n    # Check both directories with correct paths\n    paths_to_check = [\n        os.path.join(base_dir, '2023-24'),           # Foul clips\n        os.path.join(base_dir, 'non_fouls', '2023-24')  # Non-foul clips\n    ]\n    \n    for dir_path in paths_to_check:\n        if not os.path.exists(dir_path):\n            continue\n        \n        for game_folder in os.listdir(dir_path):\n            game_path = os.path.join(dir_path, game_folder)\n            if not os.path.isdir(game_path):\n                continue\n                \n            for filename in os.listdir(game_path):\n                match = re.match(r'(\\d+)_(\\d+)_frame_\\d+\\.jpg', filename)\n                if match:\n                    game_id, event_id = match.groups()\n                    clips.add(f\"{game_id}_{event_id}\")\n    \n    return len(clips)\n\n# Check existing\nnum_existing = count_clips_in_dir(FRAME_DIR)\nexpected_total = 2214  # 1213 fouls + 1001 non-fouls\n\nprint(f\"Current: {num_existing} / {expected_total} clips\")\n\nif num_existing >= 2200:  # Allow reasonable margin for clips with varying frame counts\n    print(f\"\u2713 Download appears complete, skipping\")\nelse:\n    print(f\"Syncing from S3 (output suppressed for speed)...\\n\")\n    \n    start_time = time.time()\n    \n    # Download with no output (fastest)\n    !aws s3 sync {S3_BUCKET} {FRAME_DIR} --region {AWS_REGION} --no-progress --only-show-errors\n    \n    # Final count\n    final_count = count_clips_in_dir(FRAME_DIR)\n    elapsed = time.time() - start_time\n    \n    print(f\"\\n\u2713 Sync complete: {final_count} / {expected_total} clips in {elapsed/60:.1f} min\")\n    \n    if final_count < 2200:\n        print(f\"\u26a0\ufe0f  Only {final_count} clips - check S3 or re-run to resume\")\n    else:\n        print(f\"\u2713 Run Cell 5 to reorganize frames\")"
  },
  {
   "cell_type": "code",
   "source": "import os\nimport re\nimport shutil\nfrom tqdm import tqdm\n\nTRAINING_DIR = f'{DRIVE_ROOT}/frames_training'\n\n# Check if already reorganized\nif os.path.exists(TRAINING_DIR):\n    existing_clips = [d for d in os.listdir(TRAINING_DIR) if os.path.isdir(os.path.join(TRAINING_DIR, d))]\n    print(f\"\u2713 Training frames already reorganized!\")\n    print(f\"\u2713 Found {len(existing_clips)} clips in {TRAINING_DIR}\")\n    print(f\"\\nSkipping reorganization. Delete {TRAINING_DIR} to re-run.\")\nelse:\n    print(\"=\"*80)\n    print(\"REORGANIZING FRAMES FOR TRAINING\")\n    print(\"=\"*80)\n    print(f\"Source: {FRAME_DIR}\")\n    print(f\"Destination: {TRAINING_DIR}\")\n    print(f\"\\nProcessing both foul and non-foul clips...\")\n    print(f\"Converting: {'{game_id}/{game_id}_{event_id}_frame_X.jpg'}\")\n    print(f\"To:         {'{game_id}_{event_id}/XXXXXX.jpg'}\")\n    print()\n    \n    os.makedirs(TRAINING_DIR, exist_ok=True)\n    \n    clips_created = 0\n    frames_copied = 0\n    \n    # Process both foul and non-foul directories with correct paths\n    sources = [\n        ('2023-24', os.path.join(FRAME_DIR, '2023-24')),\n        ('non_fouls', os.path.join(FRAME_DIR, 'non_fouls', '2023-24'))\n    ]\n    \n    for label, source_dir in sources:\n        if not os.path.exists(source_dir):\n            print(f\"Skipping {label} (not found)\")\n            continue\n        \n        # Get list of all game folders\n        game_folders = [d for d in os.listdir(source_dir) if os.path.isdir(os.path.join(source_dir, d))]\n        \n        print(f\"Processing {len(game_folders)} folders from {label}...\")\n        \n        for game_folder in tqdm(game_folders, desc=label):\n            game_path = os.path.join(source_dir, game_folder)\n            \n            for filename in os.listdir(game_path):\n                # Parse: {game_id}_{event_id}_frame_{idx}.jpg\n                match = re.match(r'(\\d+)_(\\d+)_frame_(\\d+)\\.jpg', filename)\n                if match:\n                    game_id, event_id, frame_idx = match.groups()\n                    \n                    # Create clip directory\n                    clip_dir = os.path.join(TRAINING_DIR, f\"{game_id}_{event_id}\")\n                    if not os.path.exists(clip_dir):\n                        os.makedirs(clip_dir, exist_ok=True)\n                        clips_created += 1\n                    \n                    # Copy frame with new name: XXXXXX.jpg (6-digit zero-padded)\n                    src_file = os.path.join(game_path, filename)\n                    dst_file = os.path.join(clip_dir, f\"{int(frame_idx):06d}.jpg\")\n                    shutil.copy2(src_file, dst_file)\n                    frames_copied += 1\n    \n    print(f\"\\n\u2713 Reorganization complete!\")\n    print(f\"  Clips created: {clips_created}\")\n    print(f\"  Frames copied: {frames_copied}\")\n    print(f\"  Location: {TRAINING_DIR}\")\n    \n    # Verify\n    !du -sh {TRAINING_DIR}\n    \n    print(f\"\\n\u2713 Ready for training! Use this path in Cell 7: {TRAINING_DIR}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 5: Reorganize Frames for Training\n\n**IMPORTANT: Run this after Cell 4 completes!**\n\nThe downloaded S3 structure has frames organized by game, but training expects each clip in its own folder.\n\nThis cell:\n- Reorganizes frames from `2023-24/{game_id}/{game_id}_{event_id}_frame_X.jpg`\n- To training format: `{game_id}_{event_id}/XXXXXX.jpg`\n- Takes ~5-10 minutes to reorganize all clips\n- Creates new directory: `frames_training/`\n\n\u26a0\ufe0f **This only needs to run once** after initial download.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 5.5: Copy Frames to Local Storage (FAST I/O)\n\n**IMPORTANT: Run this for much faster training!**\n\nGoogle Drive I/O is very slow (5+ sec/batch). Copying frames to local Colab storage provides 10-20x faster training:\n- **Drive I/O:** ~3+ hours per epoch\n- **Local I/O:** ~15-20 minutes per epoch\n\nThis one-time copy takes ~10 minutes but saves hours during training.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import os\nimport shutil\nimport time\n\n# Paths\nDRIVE_TRAINING = f'{DRIVE_ROOT}/frames_training'\nLOCAL_TRAINING = '/content/frames_training'\n\n# Check if already copied\nif os.path.exists(LOCAL_TRAINING):\n    clips = [d for d in os.listdir(LOCAL_TRAINING) if os.path.isdir(os.path.join(LOCAL_TRAINING, d))]\n    print(f\"\u2713 Frames already in local storage!\")\n    print(f\"\u2713 Found {len(clips)} clips in {LOCAL_TRAINING}\")\n    print(f\"\\nSkipping copy. Delete {LOCAL_TRAINING} to re-copy.\")\nelse:\n    print(\"=\"*80)\n    print(\"COPYING FRAMES TO LOCAL STORAGE\")\n    print(\"=\"*80)\n    print(f\"Source: {DRIVE_TRAINING} (Google Drive - slow)\")\n    print(f\"Dest:   {LOCAL_TRAINING} (Local SSD - fast)\")\n    print(\"\\nThis takes ~10 minutes but makes training 10-20x faster!\")\n    print(\"=\"*80)\n    \n    if not os.path.exists(DRIVE_TRAINING):\n        print(f\"\\n\u274c Error: {DRIVE_TRAINING} not found\")\n        print(\"Run Cell 5 first to reorganize frames in Drive\")\n    else:\n        start = time.time()\n        \n        print(\"\\nCopying frames...\")\n        shutil.copytree(DRIVE_TRAINING, LOCAL_TRAINING)\n        \n        elapsed = time.time() - start\n        clips = [d for d in os.listdir(LOCAL_TRAINING) if os.path.isdir(os.path.join(LOCAL_TRAINING, d))]\n        \n        print(f\"\\n\u2713 Copy complete in {elapsed/60:.1f} minutes!\")\n        print(f\"\u2713 Copied {len(clips)} clips to local storage\")\n        print(f\"\u2713 Training will use fast local I/O\")\n        \n        # Verify size\n        !du -sh {LOCAL_TRAINING}\n        \n        print(f\"\\n\u2713 Ready for fast training!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Clone Repository\n",
    "\n",
    "Clone the basketball foul detection training code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Clone repository if not already present\n",
    "REPO_DIR = '/content/basketball_foul_detection'\n",
    "\n",
    "if os.path.exists(REPO_DIR):\n",
    "    print(f\"\u2713 Repository already exists at {REPO_DIR}\")\n",
    "    %cd {REPO_DIR}\n",
    "    !git pull origin main 2>/dev/null || echo \"(git pull skipped)\"\n",
    "else:\n",
    "    print(\"Cloning repository...\")\n",
    "    !git clone https://github.com/githubhomie/basketball_foul_detection.git {REPO_DIR}\n",
    "    %cd {REPO_DIR}\n",
    "\n",
    "# Install dependencies\n",
    "print(\"\\nInstalling dependencies...\")\n",
    "!pip install -q -r requirements.txt\n",
    "\n",
    "# Verify critical files exist\n",
    "print(\"\\nVerifying project structure...\")\n",
    "critical_files = [\n",
    "    'train_e2e.py',\n",
    "    'data/basketball/train.json',\n",
    "    'data/basketball/val.json',\n",
    "    'data/basketball/test.json',\n",
    "    'data/basketball/class.txt'\n",
    "]\n",
    "\n",
    "all_present = True\n",
    "for file in critical_files:\n",
    "    if os.path.exists(file):\n",
    "        print(f\"  \u2713 {file}\")\n",
    "    else:\n",
    "        print(f\"  \u274c {file} - MISSING!\")\n",
    "        all_present = False\n",
    "\n",
    "if all_present:\n",
    "    print(\"\\n\u2713 All critical files present!\")\n",
    "    print(f\"\u2713 Working directory: {os.getcwd()}\")\n",
    "else:\n",
    "    print(\"\\n\u274c Some files are missing. Check repository.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Verify Setup\n",
    "\n",
    "**Run this before training to verify everything is ready.**\n",
    "\n",
    "This cell checks:\n",
    "- GPU is available\n",
    "- Frames are in Drive\n",
    "- Dataset files are correct\n",
    "- All dependencies are installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os",
    "import time",
    "from datetime import datetime",
    "",
    "# Training configuration",
    "DATASET = \"basketball\"",
    "MODEL_ARCH = \"rny002_gsm\"  # RegNet-Y 200MF + Gated Shift Module",
    "TEMPORAL_ARCH = \"gru\"      # Bidirectional GRU",
    "",
    "# Hyperparameters - UPDATED CONFIG",
    "BATCH_SIZE = 24       # Optimized for A100 (use 16 if OOM)",
    "CLIP_LEN = 30         # 30 frames per clip",
    "NUM_EPOCHS = 50       # Total training epochs",
    "LEARNING_RATE = 0.001 # Initial learning rate (with warmup)",
    "CROP_DIM = 224        # Input image size",
    "",
    "# Use LOCAL frames directory for FAST training (not Drive!)",
    "TRAINING_FRAME_DIR = '/content/frames_training'",
    "",
    "# Save directory in Drive (persistent)",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")",
    "SAVE_DIR = f\"{CHECKPOINT_DIR}/basketball_colab_{timestamp}\"",
    "os.makedirs(SAVE_DIR, exist_ok=True)",
    "",
    "print(\"=\"*80)",
    "print(\"NBA BASKETBALL FOUL DETECTION TRAINING\")",
    "print(\"=\"*80)",
    "print(f\"Dataset:        {DATASET}\")",
    "print(f\"Frame dir:      {TRAINING_FRAME_DIR} (local SSD - FAST)\")",
    "print(f\"Model:          {MODEL_ARCH} + {TEMPORAL_ARCH}\")",
    "print(f\"Clip length:    {CLIP_LEN} frames\")",
    "print(f\"Batch size:     {BATCH_SIZE}\")",
    "print(f\"Epochs:         {NUM_EPOCHS}\")",
    "print(f\"Learning rate:  {LEARNING_RATE}\")",
    "print(f\"Mixup:          False (disabled for single-frame labels)\")",
    "print(f\"Dilate len:     1 (\u00b11 frame tolerance)\")",
    "print(f\"FG upsample:    0.5 (50% clips contain events)\")",
    "print(f\"Start val:      Epoch 5 (early mAP)\")",
    "print(f\"Save dir:       {SAVE_DIR}\")",
    "print(f\"Storage:        Frames=Local (fast), Checkpoints=Drive (persistent)\")",
    "print(\"=\"*80)",
    "print()",
    "print(\"Expected time: ~6-8 hours (~8-10 min/epoch \u00d7 50 epochs)\")",
    "print(\"Keep this tab open or training will stop!\")",
    "print(\"If Colab disconnects, use Cell 18 to resume.\")",
    "print()",
    "print(\"Starting training...\\n\")",
    "",
    "start_time = time.time()",
    "",
    "# Run training - UPDATED CONFIG",
    "!python3 train_e2e.py \"{DATASET}\" \"{TRAINING_FRAME_DIR}\" \\",
    "    -m \"{MODEL_ARCH}\" \\",
    "    -t \"{TEMPORAL_ARCH}\" \\",
    "    -s \"{SAVE_DIR}\" \\",
    "    --clip_len {CLIP_LEN} \\",
    "    --crop_dim {CROP_DIM} \\",
    "    --batch_size {BATCH_SIZE} \\",
    "    --num_epochs {NUM_EPOCHS} \\",
    "    --learning_rate {LEARNING_RATE} \\",
    "    --mixup False \\",
    "    --criterion map \\",
    "    --dilate_len 1 \\",
    "    --fg_upsample 0.5 \\",
    "    --start_val_epoch 5 \\",
    "    --warm_up_epochs 3",
    "",
    "elapsed = time.time() - start_time",
    "print(f\"\\n\\n{'='*80}\")",
    "print(f\"Training completed in {elapsed/3600:.1f} hours!\")",
    "print(f\"{'='*80}\")",
    "print(f\"\\nResults saved to: {SAVE_DIR}\")",
    "print(f\"\\nCheckpoints are in Google Drive (persistent).\")",
    "print(f\"You can close Colab now.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Start Training\n",
    "\n",
    "**Starts training the foul detection model.**\n",
    "\n",
    "### Configuration:\n",
    "- **Model:** RegNet-Y 200MF + Gated Shift Module + BiGRU\n",
    "- **Batch size:** 8 (optimized for V100/A100, reduce to 6 if OOM on T4)\n",
    "- **Epochs:** 50\n",
    "- **Learning rate:** 0.001 with warmup and cosine decay\n",
    "- **Clip length:** 30 frames\n",
    "- **Input size:** 224\u00d7224\n",
    "\n",
    "### Timeline:\n",
    "- **Each epoch:** ~20-25 minutes (depends on GPU)\n",
    "- **Total time:** ~14-17 hours\n",
    "- **Colab Pro:** 24-hour session should complete in one go\n",
    "\n",
    "### During Training:\n",
    "- Checkpoints saved to Drive automatically\n",
    "- You can close laptop and check back later\n",
    "- If session disconnects, use Cell 8 to resume\n",
    "\n",
    "\u26a0\ufe0f **Keep this tab open** or training will stop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport time\nfrom glob import glob\nfrom datetime import datetime\n\n# Use LOCAL frames directory (FAST I/O)\nTRAINING_FRAME_DIR = '/content/frames_training'\n\n# Find available checkpoints in Drive\ncheckpoint_dirs = glob(os.path.join(CHECKPOINT_DIR, 'basketball_*'))\n\nif not checkpoint_dirs:\n    print(\"\u274c No checkpoints found in Google Drive.\")\n    print(f\"Expected location: {CHECKPOINT_DIR}\")\n    print(\"\\nMake sure Cell 7 (training) created checkpoints.\")\n    print(\"Or run Cell 7 to start fresh training.\")\nelse:\n    print(\"Available checkpoints in Drive:\")\n    for i, ckpt_dir in enumerate(sorted(checkpoint_dirs)):\n        name = os.path.basename(ckpt_dir)\n        files = os.listdir(ckpt_dir)\n        checkpoint_files = [f for f in files if f.startswith('checkpoint_') and f.endswith('.pt')]\n        print(f\"  [{i}] {name} ({len(checkpoint_files)} checkpoint files)\")\n    \n    # Auto-select most recent checkpoint\n    selected_checkpoint = sorted(checkpoint_dirs)[-1]\n    checkpoint_name = os.path.basename(selected_checkpoint)\n    \n    print(f\"\\nResuming from: {checkpoint_name}\")\n    print(f\"Location: {selected_checkpoint}\")\n    \n    # Training configuration (same as Cell 7)\n    DATASET = \"basketball\"\n    MODEL_ARCH = \"rny002_gsm\"\n    TEMPORAL_ARCH = \"gru\"\n    BATCH_SIZE = 8\n    CLIP_LEN = 30\n    NUM_EPOCHS = 50\n    LEARNING_RATE = 0.001\n    CROP_DIM = 224\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"RESUMING TRAINING\")\n    print(\"=\"*80)\n    print(f\"Checkpoint: {checkpoint_name}\")\n    print(f\"Frame dir:  {TRAINING_FRAME_DIR} (local SSD - FAST)\")\n    print(f\"Save dir:   {selected_checkpoint}\")\n    print(\"=\"*80)\n    print()\n    \n    start_time = time.time()\n    \n    # Resume training - using LOCAL frames for fast I/O\n    !python3 train_e2e.py \"{DATASET}\" \"{TRAINING_FRAME_DIR}\" \\\n        -m \"{MODEL_ARCH}\" \\\n        -t \"{TEMPORAL_ARCH}\" \\\n        -s \"{selected_checkpoint}\" \\\n        --clip_len {CLIP_LEN} \\\n        --crop_dim {CROP_DIM} \\\n        --batch_size {BATCH_SIZE} \\\n        --num_epochs {NUM_EPOCHS} \\\n        --learning_rate {LEARNING_RATE} \\\n        --mixup True \\\n        --criterion map \\\n        --dilate_len 0 \\\n        --warm_up_epochs 3 \\\n        --resume\n    \n    elapsed = time.time() - start_time\n    print(f\"\\n\\n{'='*80}\")\n    print(f\"\u2713 Training session completed in {elapsed/3600:.1f} hours!\")\n    print(f\"{'='*80}\")\n    print(f\"\\nResults saved to: {selected_checkpoint}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Resume Training from Checkpoint\n",
    "\n",
    "**Use this if your Colab session disconnected during training.**\n",
    "\n",
    "### When to use:\n",
    "- Colab session timed out or disconnected\n",
    "- You manually stopped training and want to continue\n",
    "- Starting a new session to finish remaining epochs\n",
    "\n",
    "### Prerequisites:\n",
    "1. Start a new Colab session\n",
    "2. Run Cells 1-6 first (setup, but skip frame download if already in Drive)\n",
    "3. Then run this cell to resume\n",
    "\n",
    "This will:\n",
    "- Find your latest checkpoint in Drive\n",
    "- Resume training from the last completed epoch\n",
    "- Continue until all epochs are done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport time\nfrom glob import glob\nfrom datetime import datetime\n\n# Use reorganized frames directory\nTRAINING_FRAME_DIR = f'{DRIVE_ROOT}/frames_training'\n\n# Find available checkpoints in Drive\ncheckpoint_dirs = glob(os.path.join(CHECKPOINT_DIR, 'basketball_*'))\n\nif not checkpoint_dirs:\n    print(\"\u274c No checkpoints found in Google Drive.\")\n    print(f\"Expected location: {CHECKPOINT_DIR}\")\n    print(\"\\nMake sure Cell 7 (training) created checkpoints.\")\n    print(\"Or run Cell 7 to start fresh training.\")\nelse:\n    print(\"Available checkpoints in Drive:\")\n    for i, ckpt_dir in enumerate(sorted(checkpoint_dirs)):\n        name = os.path.basename(ckpt_dir)\n        files = os.listdir(ckpt_dir)\n        checkpoint_files = [f for f in files if f.startswith('checkpoint_') and f.endswith('.pt')]\n        print(f\"  [{i}] {name} ({len(checkpoint_files)} checkpoint files)\")\n    \n    # Auto-select most recent checkpoint\n    selected_checkpoint = sorted(checkpoint_dirs)[-1]\n    checkpoint_name = os.path.basename(selected_checkpoint)\n    \n    print(f\"\\nResuming from: {checkpoint_name}\")\n    print(f\"Location: {selected_checkpoint}\")\n    \n    # Training configuration (same as Cell 7)\n    DATASET = \"basketball\"\n    MODEL_ARCH = \"rny002_gsm\"\n    TEMPORAL_ARCH = \"gru\"\n    BATCH_SIZE = 8\n    CLIP_LEN = 30\n    NUM_EPOCHS = 50\n    LEARNING_RATE = 0.001\n    CROP_DIM = 224\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"RESUMING TRAINING\")\n    print(\"=\"*80)\n    print(f\"Checkpoint: {checkpoint_name}\")\n    print(f\"Frame dir:  {TRAINING_FRAME_DIR} (reorganized)\")\n    print(f\"Save dir:   {selected_checkpoint}\")\n    print(\"=\"*80)\n    print()\n    \n    start_time = time.time()\n    \n    # Resume training - using reorganized frames from Drive\n    !python3 train_e2e.py \"{DATASET}\" \"{TRAINING_FRAME_DIR}\" \\\n        -m \"{MODEL_ARCH}\" \\\n        -t \"{TEMPORAL_ARCH}\" \\\n        -s \"{selected_checkpoint}\" \\\n        --clip_len {CLIP_LEN} \\\n        --crop_dim {CROP_DIM} \\\n        --batch_size {BATCH_SIZE} \\\n        --num_epochs {NUM_EPOCHS} \\\n        --learning_rate {LEARNING_RATE} \\\n        --mixup True \\\n        --criterion map \\\n        --dilate_len 0 \\\n        --warm_up_epochs 3 \\\n        --resume\n    \n    elapsed = time.time() - start_time\n    print(f\"\\n\\n{'='*80}\")\n    print(f\"\u2713 Training session completed in {elapsed/3600:.1f} hours!\")\n    print(f\"{'='*80}\")\n    print(f\"\\nResults saved to: {selected_checkpoint}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Check Training Progress\n",
    "\n",
    "View training metrics and progress without interrupting training.\n",
    "\n",
    "Run this anytime to check:\n",
    "- Which checkpoints have been saved\n",
    "- Training loss history\n",
    "- Validation mAP (if computed)\n",
    "- Current training status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from glob import glob\n",
    "\n",
    "# Find checkpoint directories\n",
    "checkpoint_dirs = sorted(glob(os.path.join(CHECKPOINT_DIR, 'basketball_*')))\n",
    "\n",
    "if not checkpoint_dirs:\n",
    "    print(\"No training runs found yet.\")\n",
    "    print(\"Start training with Cell 7.\")\n",
    "else:\n",
    "    latest_run = checkpoint_dirs[-1]\n",
    "    run_name = os.path.basename(latest_run)\n",
    "    \n",
    "    print(f\"Latest training run: {run_name}\")\n",
    "    print(f\"Location: {latest_run}\\n\")\n",
    "    \n",
    "    # List checkpoint files\n",
    "    checkpoint_files = sorted([f for f in os.listdir(latest_run) \n",
    "                               if f.startswith('checkpoint_') and f.endswith('.pt')])\n",
    "    \n",
    "    if checkpoint_files:\n",
    "        print(f\"Saved checkpoints ({len(checkpoint_files)}):\")\n",
    "        for ckpt in checkpoint_files:\n",
    "            ckpt_path = os.path.join(latest_run, ckpt)\n",
    "            size_mb = os.path.getsize(ckpt_path) / (1024*1024)\n",
    "            print(f\"  {ckpt} ({size_mb:.1f} MB)\")\n",
    "    else:\n",
    "        print(\"No checkpoint files saved yet (training may be starting...)\")\n",
    "    \n",
    "    # Show training history if available\n",
    "    loss_file = os.path.join(latest_run, 'loss.json')\n",
    "    if os.path.exists(loss_file):\n",
    "        print(f\"\\nTraining history:\")\n",
    "        with open(loss_file) as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        # Show last 10 epochs\n",
    "        print(f\"  (showing last 10 epochs)\\n\")\n",
    "        for line in lines[-20:]:  # Last 20 lines (train+val per epoch)\n",
    "            data = json.loads(line)\n",
    "            split = data['split']\n",
    "            epoch = data['epoch']\n",
    "            loss = data['loss']\n",
    "            map_score = data.get('mAP', 'N/A')\n",
    "            if map_score != 'N/A':\n",
    "                print(f\"  Epoch {epoch:3d} [{split:5s}]: Loss={loss:.4f}, mAP={map_score:.4f}\")\n",
    "            else:\n",
    "                print(f\"  Epoch {epoch:3d} [{split:5s}]: Loss={loss:.4f}\")\n",
    "    else:\n",
    "        print(f\"\\nNo loss.json found yet (training may be in first epoch)\")\n",
    "    \n",
    "    # Show directory listing\n",
    "    print(f\"\\nFull directory contents:\")\n",
    "    !ls -lh \"{latest_run}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### Session disconnected during training\n",
    "1. Start new Colab session\n",
    "2. Run Cells 1-3 (setup + mount Drive)\n",
    "3. Skip Cell 4 (frames already in Drive)\n",
    "4. Run Cell 5 (clone repo)\n",
    "5. Run Cell 8 (resume training)\n",
    "\n",
    "### Out of memory error\n",
    "- Edit Cell 7, change `BATCH_SIZE = 8` to `BATCH_SIZE = 6` or `BATCH_SIZE = 4`\n",
    "- Restart runtime and run Cell 7 again\n",
    "\n",
    "### Frames not downloading\n",
    "- Check AWS credentials in Cell 3\n",
    "- Verify S3 bucket access: `!aws s3 ls s3://nba-foul-dataset-oh/`\n",
    "- Check internet connection\n",
    "- Re-run Cell 4 (aws s3 sync will resume interrupted downloads)\n",
    "\n",
    "### GPU not detected\n",
    "- Runtime \u2192 Change runtime type \u2192 GPU\n",
    "- If still no GPU, try Runtime \u2192 Factory reset runtime\n",
    "\n",
    "### Training is slow\n",
    "- Check GPU usage in Cell 1 (should show V100 or A100 with Colab Pro)\n",
    "- T4 GPU is slower (~25-30 min/epoch)\n",
    "- V100/A100 is faster (~15-20 min/epoch)\n",
    "\n",
    "---\n",
    "\n",
    "## Expected Results\n",
    "\n",
    "**Success criteria:**\n",
    "- Overall mAP @ tolerance=2: **\u22650.60** (good), **\u22650.65** (excellent)\n",
    "- Training time: ~14-17 hours\n",
    "- Final test mAP computed automatically\n",
    "\n",
    "**After training completes:**\n",
    "- Checkpoints are in Drive: `/content/drive/MyDrive/nba_foul_training/checkpoints/`\n",
    "- Best model saved as `checkpoint_best.pt`\n",
    "- Training history in `loss.json`\n",
    "- Test predictions in `pred-test.*.json`\n",
    "\n",
    "You can download checkpoints from Drive to your computer or use them directly for inference."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}