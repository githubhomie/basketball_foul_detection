{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# NBA Basketball Foul Detection - Colab Training\n", "\n", "This notebook trains the E2E-Spot model for basketball foul detection on Google Colab Pro.\n", "\n", "## Setup Overview\n", "\n", "1. **Environment Setup** - Install dependencies (~2 min)\n", "2. **Mount Google Drive** - Connect persistent storage (~10 sec)\n", "3. **AWS Credentials** - Configure S3 access (~30 sec)\n", "4. **Download Frames to Drive** - One-time download of 22GB (~10-15 min)\n", "5. **Clone Repository** - Get training code (~30 sec)\n", "6. **Verify Setup** - Check everything is ready (~10 sec)\n", "7. **Start Training** - Begin training (~14-17 hours with Colab Pro)\n", "8. **Resume Training** - Continue if session disconnects\n", "\n", "## Important Notes\n", "\n", "- **Colab Pro:** 24-hour sessions (vs 12 hours free)\n", "- **Frames:** Downloaded once to Drive, persistent across sessions\n", "- **Checkpoints:** Saved directly to Drive, no separate backup needed\n", "- **Total time:** ~14-17 hours (might finish in one Colab Pro session)\n", "\n", "**Enable GPU:** Runtime \u2192 Change runtime type \u2192 GPU (T4/V100/A100)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Cell 1: Environment Setup\n", "\n", "Install PyTorch and dependencies. Run this first."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Install dependencies\n", "print(\"Installing dependencies for Colab environment...\")\n", "!pip install -q torch torchvision timm tqdm tabulate opencv-python pillow matplotlib\n", "\n", "# Verify GPU availability\n", "import torch\n", "print(f\"\\n\u2713 PyTorch version: {torch.__version__}\")\n", "print(f\"\u2713 CUDA available: {torch.cuda.is_available()}\")\n", "if torch.cuda.is_available():\n", "    print(f\"\u2713 GPU: {torch.cuda.get_device_name(0)}\")\n", "    print(f\"\u2713 GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n", "    # Colab Pro may give you V100 (32GB) or A100 (40GB)\n", "else:\n", "    print(\"\\n\u26a0\ufe0f  WARNING: No GPU detected!\")\n", "    print(\"Please enable GPU: Runtime \u2192 Change runtime type \u2192 GPU\")\n", "\n", "print(\"\\n\u2713 Environment setup complete!\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Cell 2: Mount Google Drive\n", "\n", "Connect Google Drive for persistent storage of frames and checkpoints."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from google.colab import drive\n", "import os\n", "\n", "# Mount Google Drive\n", "drive.mount('/content/drive')\n", "\n", "# Create directories in Drive (persistent across sessions)\n", "DRIVE_ROOT = '/content/drive/MyDrive/nba_foul_training'\n", "FRAME_DIR = f'{DRIVE_ROOT}/frames'\n", "CHECKPOINT_DIR = f'{DRIVE_ROOT}/checkpoints'\n", "\n", "os.makedirs(DRIVE_ROOT, exist_ok=True)\n", "os.makedirs(FRAME_DIR, exist_ok=True)\n", "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n", "\n", "print(f\"\\n\u2713 Google Drive mounted successfully\")\n", "print(f\"\u2713 Frames will be stored in: {FRAME_DIR}\")\n", "print(f\"\u2713 Checkpoints will be saved to: {CHECKPOINT_DIR}\")\n", "print(f\"\\nCurrent Drive contents:\")\n", "!ls -lh \"$DRIVE_ROOT\" 2>/dev/null || echo \"  (empty)\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Cell 3: AWS Credentials\n", "\n", "Configure AWS credentials to download frames from S3.\n", "\n", "**Security:** Credentials are stored only in this session (temporary)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os\n", "from getpass import getpass\n", "\n", "print(\"Enter your AWS credentials (input is hidden):\")\n", "print(\"These are used ONLY in this Colab session to download frames from S3.\")\n", "print(\"Your local AWS config is NOT affected.\\n\")\n", "\n", "AWS_ACCESS_KEY_ID = getpass(\"AWS Access Key ID: \")\n", "AWS_SECRET_ACCESS_KEY = getpass(\"AWS Secret Access Key: \")\n", "AWS_REGION = input(\"AWS Region (default: us-east-2): \") or \"us-east-2\"\n", "\n", "# Set environment variables (temporary, session-only)\n", "os.environ['AWS_ACCESS_KEY_ID'] = AWS_ACCESS_KEY_ID\n", "os.environ['AWS_SECRET_ACCESS_KEY'] = AWS_SECRET_ACCESS_KEY\n", "os.environ['AWS_DEFAULT_REGION'] = AWS_REGION\n", "\n", "# Install AWS CLI\n", "print(\"\\nInstalling AWS CLI...\")\n", "!pip install -q awscli\n", "\n", "# Test credentials (without displaying them)\n", "import subprocess\n", "result = subprocess.run(['aws', 's3', 'ls', 's3://nba-foul-dataset-oh/'], \n", "                       capture_output=True, text=True)\n", "if result.returncode == 0:\n", "    print(\"\\n\u2713 AWS credentials verified successfully!\")\n", "    print(\"\u2713 S3 bucket access confirmed\")\n", "else:\n", "    print(\"\\n\u274c AWS credential verification failed. Please check your keys.\")\n", "    print(f\"Error: {result.stderr}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Cell 4: Download Frames to Google Drive\n", "\n", "**Downloads 22GB of frame data from S3 directly to Google Drive.**\n", "\n", "- **First time:** Takes ~10-15 minutes\n", "- **Subsequent sessions:** Skipped (frames already in Drive)\n", "- **Persistent:** Frames stay in Drive across sessions\n", "\n", "\u26a0\ufe0f **This is the critical step.** Make sure it completes successfully."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "import os\nimport re\nimport time\n\nS3_BUCKET = 's3://nba-foul-dataset-oh/frames/'\n\ndef count_clips_in_dir(base_dir):\n    \"\"\"Count unique clips from both foul and non-foul directories\"\"\"\n    clips = set()\n    \n    # Check both directories with correct paths\n    paths_to_check = [\n        os.path.join(base_dir, '2023-24'),           # Foul clips\n        os.path.join(base_dir, 'non_fouls', '2023-24')  # Non-foul clips\n    ]\n    \n    for dir_path in paths_to_check:\n        if not os.path.exists(dir_path):\n            continue\n        \n        for game_folder in os.listdir(dir_path):\n            game_path = os.path.join(dir_path, game_folder)\n            if not os.path.isdir(game_path):\n                continue\n                \n            for filename in os.listdir(game_path):\n                match = re.match(r'(\\d+)_(\\d+)_frame_\\d+\\.jpg', filename)\n                if match:\n                    game_id, event_id = match.groups()\n                    clips.add(f\"{game_id}_{event_id}\")\n    \n    return len(clips)\n\n# Check existing\nnum_existing = count_clips_in_dir(FRAME_DIR)\nexpected_total = 2214  # 1213 fouls + 1001 non-fouls\n\nprint(f\"Current: {num_existing} / {expected_total} clips\")\n\nif num_existing >= 2200:  # Allow reasonable margin for clips with varying frame counts\n    print(f\"\u2713 Download appears complete, skipping\")\nelse:\n    print(f\"Syncing from S3 (output suppressed for speed)...\\n\")\n    \n    start_time = time.time()\n    \n    # Download with no output (fastest)\n    !aws s3 sync {S3_BUCKET} {FRAME_DIR} --region {AWS_REGION} --no-progress --only-show-errors\n    \n    # Final count\n    final_count = count_clips_in_dir(FRAME_DIR)\n    elapsed = time.time() - start_time\n    \n    print(f\"\\n\u2713 Sync complete: {final_count} / {expected_total} clips in {elapsed/60:.1f} min\")\n    \n    if final_count < 2200:\n        print(f\"\u26a0\ufe0f  Only {final_count} clips - check S3 or re-run to resume\")\n    else:\n        print(f\"\u2713 Run Cell 5 to reorganize frames\")"}, {"cell_type": "code", "source": "import os\nimport re\nimport shutil\nfrom tqdm import tqdm\n\nTRAINING_DIR = f'{DRIVE_ROOT}/frames_training'\n\n# Check if already reorganized\nif os.path.exists(TRAINING_DIR):\n    existing_clips = [d for d in os.listdir(TRAINING_DIR) if os.path.isdir(os.path.join(TRAINING_DIR, d))]\n    print(f\"\u2713 Training frames already reorganized!\")\n    print(f\"\u2713 Found {len(existing_clips)} clips in {TRAINING_DIR}\")\n    print(f\"\\nSkipping reorganization. Delete {TRAINING_DIR} to re-run.\")\nelse:\n    print(\"=\"*80)\n    print(\"REORGANIZING FRAMES FOR TRAINING\")\n    print(\"=\"*80)\n    print(f\"Source: {FRAME_DIR}\")\n    print(f\"Destination: {TRAINING_DIR}\")\n    print(f\"\\nProcessing both foul and non-foul clips...\")\n    print(f\"Converting: {'{game_id}/{game_id}_{event_id}_frame_X.jpg'}\")\n    print(f\"To:         {'{game_id}_{event_id}/XXXXXX.jpg'}\")\n    print()\n    \n    os.makedirs(TRAINING_DIR, exist_ok=True)\n    \n    clips_created = 0\n    frames_copied = 0\n    \n    # Process both foul and non-foul directories with correct paths\n    sources = [\n        ('2023-24', os.path.join(FRAME_DIR, '2023-24')),\n        ('non_fouls', os.path.join(FRAME_DIR, 'non_fouls', '2023-24'))\n    ]\n    \n    for label, source_dir in sources:\n        if not os.path.exists(source_dir):\n            print(f\"Skipping {label} (not found)\")\n            continue\n        \n        # Get list of all game folders\n        game_folders = [d for d in os.listdir(source_dir) if os.path.isdir(os.path.join(source_dir, d))]\n        \n        print(f\"Processing {len(game_folders)} folders from {label}...\")\n        \n        for game_folder in tqdm(game_folders, desc=label):\n            game_path = os.path.join(source_dir, game_folder)\n            \n            for filename in os.listdir(game_path):\n                # Parse: {game_id}_{event_id}_frame_{idx}.jpg\n                match = re.match(r'(\\d+)_(\\d+)_frame_(\\d+)\\.jpg', filename)\n                if match:\n                    game_id, event_id, frame_idx = match.groups()\n                    \n                    # Create clip directory\n                    clip_dir = os.path.join(TRAINING_DIR, f\"{game_id}_{event_id}\")\n                    if not os.path.exists(clip_dir):\n                        os.makedirs(clip_dir, exist_ok=True)\n                        clips_created += 1\n                    \n                    # Copy frame with new name: XXXXXX.jpg (6-digit zero-padded)\n                    src_file = os.path.join(game_path, filename)\n                    dst_file = os.path.join(clip_dir, f\"{int(frame_idx):06d}.jpg\")\n                    shutil.copy2(src_file, dst_file)\n                    frames_copied += 1\n    \n    print(f\"\\n\u2713 Reorganization complete!\")\n    print(f\"  Clips created: {clips_created}\")\n    print(f\"  Frames copied: {frames_copied}\")\n    print(f\"  Location: {TRAINING_DIR}\")\n    \n    # Verify\n    !du -sh {TRAINING_DIR}\n    \n    print(f\"\\n\u2713 Ready for training! Use this path in Cell 7: {TRAINING_DIR}\")", "metadata": {}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "## Cell 5: Reorganize Frames for Training\n\n**IMPORTANT: Run this after Cell 4 completes!**\n\nThe downloaded S3 structure has frames organized by game, but training expects each clip in its own folder.\n\nThis cell:\n- Reorganizes frames from `2023-24/{game_id}/{game_id}_{event_id}_frame_X.jpg`\n- To training format: `{game_id}_{event_id}/XXXXXX.jpg`\n- Takes ~5-10 minutes to reorganize all clips\n- Creates new directory: `frames_training/`\n\n\u26a0\ufe0f **This only needs to run once** after initial download.", "metadata": {}}, {"cell_type": "markdown", "source": "## Cell 5.5: Copy Frames to Local Storage (FAST I/O)\n\n**IMPORTANT: Run this for much faster training!**\n\nGoogle Drive I/O is very slow (5+ sec/batch). Copying frames to local Colab storage provides 10-20x faster training:\n- **Drive I/O:** ~3+ hours per epoch\n- **Local I/O:** ~15-20 minutes per epoch\n\nThis one-time copy takes ~10 minutes but saves hours during training.", "metadata": {}}, {"cell_type": "code", "source": "import os\nimport shutil\nimport time\n\n# Paths\nDRIVE_TRAINING = f'{DRIVE_ROOT}/frames_training'\nLOCAL_TRAINING = '/content/frames_training'\n\n# Check if already copied\nif os.path.exists(LOCAL_TRAINING):\n    clips = [d for d in os.listdir(LOCAL_TRAINING) if os.path.isdir(os.path.join(LOCAL_TRAINING, d))]\n    print(f\"\u2713 Frames already in local storage!\")\n    print(f\"\u2713 Found {len(clips)} clips in {LOCAL_TRAINING}\")\n    print(f\"\\nSkipping copy. Delete {LOCAL_TRAINING} to re-copy.\")\nelse:\n    print(\"=\"*80)\n    print(\"COPYING FRAMES TO LOCAL STORAGE\")\n    print(\"=\"*80)\n    print(f\"Source: {DRIVE_TRAINING} (Google Drive - slow)\")\n    print(f\"Dest:   {LOCAL_TRAINING} (Local SSD - fast)\")\n    print(\"\\nThis takes ~10 minutes but makes training 10-20x faster!\")\n    print(\"=\"*80)\n    \n    if not os.path.exists(DRIVE_TRAINING):\n        print(f\"\\n\u274c Error: {DRIVE_TRAINING} not found\")\n        print(\"Run Cell 5 first to reorganize frames in Drive\")\n    else:\n        start = time.time()\n        \n        print(\"\\nCopying frames...\")\n        shutil.copytree(DRIVE_TRAINING, LOCAL_TRAINING)\n        \n        elapsed = time.time() - start\n        clips = [d for d in os.listdir(LOCAL_TRAINING) if os.path.isdir(os.path.join(LOCAL_TRAINING, d))]\n        \n        print(f\"\\n\u2713 Copy complete in {elapsed/60:.1f} minutes!\")\n        print(f\"\u2713 Copied {len(clips)} clips to local storage\")\n        print(f\"\u2713 Training will use fast local I/O\")\n        \n        # Verify size\n        !du -sh {LOCAL_TRAINING}\n        \n        print(f\"\\n\u2713 Ready for fast training!\")", "metadata": {}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## Cell 5: Clone Repository\n", "\n", "Clone the basketball foul detection training code."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os\n", "\n", "# Clone repository if not already present\n", "REPO_DIR = '/content/basketball_foul_detection'\n", "\n", "if os.path.exists(REPO_DIR):\n", "    print(f\"\u2713 Repository already exists at {REPO_DIR}\")\n", "    %cd {REPO_DIR}\n", "    !git pull origin main 2>/dev/null || echo \"(git pull skipped)\"\n", "else:\n", "    print(\"Cloning repository...\")\n", "    !git clone https://github.com/githubhomie/basketball_foul_detection.git {REPO_DIR}\n", "    %cd {REPO_DIR}\n", "\n", "# Install dependencies\n", "print(\"\\nInstalling dependencies...\")\n", "!pip install -q -r requirements.txt\n", "\n", "# Verify critical files exist\n", "print(\"\\nVerifying project structure...\")\n", "critical_files = [\n", "    'train_e2e.py',\n", "    'data/basketball/train.json',\n", "    'data/basketball/val.json',\n", "    'data/basketball/test.json',\n", "    'data/basketball/class.txt'\n", "]\n", "\n", "all_present = True\n", "for file in critical_files:\n", "    if os.path.exists(file):\n", "        print(f\"  \u2713 {file}\")\n", "    else:\n", "        print(f\"  \u274c {file} - MISSING!\")\n", "        all_present = False\n", "\n", "if all_present:\n", "    print(\"\\n\u2713 All critical files present!\")\n", "    print(f\"\u2713 Working directory: {os.getcwd()}\")\n", "else:\n", "    print(\"\\n\u274c Some files are missing. Check repository.\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Cell 6: Verify Setup\n", "\n", "**Run this before training to verify everything is ready.**\n", "\n", "This cell checks:\n", "- GPU is available\n", "- Frames are in Drive\n", "- Dataset files are correct\n", "- All dependencies are installed"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os, json\n", "from pathlib import Path\n", "\n", "print('='*80)\n", "print('PRE-TRAINING VERIFICATION')\n", "print('='*80)\n", "\n", "FRAME_DIR_TRAINING = '/content/frames_training'\n", "DATA_DIR = '/content/basketball_foul_detection/data/basketball'\n", "CHECKPOINT_DIR = '/content/drive/MyDrive/nba_foul_training/checkpoints'\n", "\n", "# 1) GPU check\n", "try:\n", "    import torch\n", "    has_gpu = torch.cuda.is_available()\n", "    gpu_name = torch.cuda.get_device_name(0) if has_gpu else 'None'\n", "    gpu_mem = torch.cuda.get_device_properties(0).total_memory/1e9 if has_gpu else 0\n", "    print('[1/5] GPU Check')\n", "    if has_gpu:\n", "        print(f'  \u2713 GPU: {gpu_name}')\n", "        print(f'  \u2713 Memory: {gpu_mem:.1f} GB')\n", "    else:\n", "        print('  \u274c No GPU found')\n", "except Exception as e:\n", "    print('  \u274c GPU check failed:', e)\n", "\n", "# 2) Frames check\n", "print('\\n[2/5] Training Frames Check')\n", "if os.path.exists(FRAME_DIR_TRAINING):\n", "    clips = [d for d in os.listdir(FRAME_DIR_TRAINING) if os.path.isdir(os.path.join(FRAME_DIR_TRAINING, d))]\n", "    print(f'  \u2713 Training frame directory: {FRAME_DIR_TRAINING}')\n", "    print(f'  \u2713 Number of clips: {len(clips)}')\n", "else:\n", "    print(f'  \u274c Training frames not found at {FRAME_DIR_TRAINING}')\n", "\n", "# 3) Dataset check\n", "print('\\n[3/5] Dataset Check')\n", "missing = []\n", "for split in ['train','val','test']:\n", "    split_path = Path(DATA_DIR)/f\"{split}.json\"\n", "    if split_path.exists():\n", "        with open(split_path) as f:\n", "            data = json.load(f)\n", "        print(f'  \u2713 {split}.json: {len(data)} clips')\n", "    else:\n", "        missing.append(split_path)\n", "if missing:\n", "    print('  \u274c Missing:')\n", "    for m in missing:\n", "        print(f'    - {m}')\n", "\n", "# 4) Classes check\n", "print('\\n[4/5] Classes Check')\n", "class_path = Path(DATA_DIR)/'class.txt'\n", "if class_path.exists():\n", "    classes = [l.strip() for l in open(class_path) if l.strip()]\n", "    print(f'  \u2713 class.txt: {len(classes)} foul types')\n", "    for c in classes:\n", "        print(f'    - {c}')\n", "else:\n", "    print(f'  \u274c class.txt missing at {class_path}')\n", "\n", "# 5) Checkpoint dir\n", "print('\\n[5/5] Checkpoint Directory Check')\n", "if os.path.exists(CHECKPOINT_DIR):\n", "    ckpts = [d for d in os.listdir(CHECKPOINT_DIR) if os.path.isdir(os.path.join(CHECKPOINT_DIR, d))]\n", "    print(f'  \u2713 Checkpoint directory: {CHECKPOINT_DIR}')\n", "    print(f'  \u2139\ufe0f  Found {len(ckpts)} existing checkpoint(s)')\n", "else:\n", "    print(f'  \u274c Checkpoint directory not found: {CHECKPOINT_DIR}')\n", "\n", "print('\\n' + '='*80)\n", "print('\u2713 VERIFICATION COMPLETE - Ready to train!' if not missing else '\u26a0\ufe0f  Verification finished with issues above.')\n", "print('='*80)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Cell 7: Start Training\n", "\n", "**Starts training the foul detection model.**\n", "\n", "### Configuration:\n", "- **Model:** RegNet-Y 200MF + Gated Shift Module + BiGRU\n", "- **Batch size:** 8 (optimized for V100/A100, reduce to 6 if OOM on T4)\n", "- **Epochs:** 50\n", "- **Learning rate:** 0.001 with warmup and cosine decay\n", "- **Clip length:** 30 frames\n", "- **Input size:** 224\u00d7224\n", "\n", "### Timeline:\n", "- **Each epoch:** ~20-25 minutes (depends on GPU)\n", "- **Total time:** ~14-17 hours\n", "- **Colab Pro:** 24-hour session should complete in one go\n", "\n", "### During Training:\n", "- Checkpoints saved to Drive automatically\n", "- You can close laptop and check back later\n", "- If session disconnects, use Cell 8 to resume\n", "\n", "\u26a0\ufe0f **Keep this tab open** or training will stop!"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os, time\n", "from datetime import datetime\n", "\n", "TRAINING_FRAME_DIR = '/content/frames_training'\n", "DATASET = 'basketball'\n", "MODEL_ARCH = 'rny002_gsm'\n", "TEMPORAL_ARCH = 'gru'\n", "BATCH_SIZE = 8\n", "CLIP_LEN = 30\n", "NUM_EPOCHS = 30\n", "LEARNING_RATE = 0.0005\n", "CROP_DIM = 224\n", "\n", "# Save directory in Drive (persistent)\n", "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n", "SAVE_DIR = f\"/content/drive/MyDrive/nba_foul_training/checkpoints/basketball_colab_{timestamp}\"\n", "os.makedirs(SAVE_DIR, exist_ok=True)\n", "\n", "print('='*80)\n", "print('NBA BASKETBALL FOUL DETECTION TRAINING')\n", "print('='*80)\n", "print(f'Frame dir:      {TRAINING_FRAME_DIR}')\n", "print(f'Save dir:       {SAVE_DIR}')\n", "print(f'Batch size:     {BATCH_SIZE}')\n", "print('Mixup:          False (disabled)')\n", "print('Dilate len:     2')\n", "print('FG upsample:    0.7')\n", "print('Start val:      Epoch 5')\n", "print('LR:             0.0005')\n", "print('Weight decay:   0.01')\n", "print('='*80)\n", "print()\n", "\n", "start_time = time.time()\n", "\n", "!python3 train_e2e.py \"{DATASET}\" \"{TRAINING_FRAME_DIR}\"     -m \"{MODEL_ARCH}\"     -t \"{TEMPORAL_ARCH}\"     -s \"{SAVE_DIR}\"     --clip_len {CLIP_LEN}     --crop_dim {CROP_DIM}     --batch_size {BATCH_SIZE}     --num_epochs {NUM_EPOCHS}     --learning_rate {LEARNING_RATE}     --mixup ''     --criterion map     --dilate_len 2     --fg_upsample 0.7     --start_val_epoch 5     --warm_up_epochs 3     -j 0\n", "\n", "elapsed = time.time() - start_time\n", "print('='*80)\n", "print(f'Training completed in {elapsed/3600:.1f} hours!')\n", "print('='*80)\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Cell 8: Resume Training from Checkpoint\n", "\n", "**Use this if your Colab session disconnected during training.**\n", "\n", "### When to use:\n", "- Colab session timed out or disconnected\n", "- You manually stopped training and want to continue\n", "- Starting a new session to finish remaining epochs\n", "\n", "### Prerequisites:\n", "1. Start a new Colab session\n", "2. Run Cells 1-6 first (setup, but skip frame download if already in Drive)\n", "3. Then run this cell to resume\n", "\n", "This will:\n", "- Find your latest checkpoint in Drive\n", "- Resume training from the last completed epoch\n", "- Continue until all epochs are done"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os, time\n", "from glob import glob\n", "from datetime import datetime\n", "\n", "LOCAL_FRAMES = '/content/frames_training'\n", "DRIVE_FRAMES = f'{DRIVE_ROOT}/frames_training'\n", "TRAINING_FRAME_DIR = LOCAL_FRAMES if os.path.exists(LOCAL_FRAMES) else DRIVE_FRAMES\n", "\n", "checkpoint_dirs = glob(os.path.join(CHECKPOINT_DIR, 'basketball_*'))\n", "\n", "if not checkpoint_dirs:\n", "    print('\u274c No checkpoints found in Google Drive.')\n", "    print(f'Expected location: {CHECKPOINT_DIR}')\n", "    print('Make sure Cell 7 created checkpoints or run Cell 7 to start fresh.')\n", "else:\n", "    print('Available checkpoints in Drive:')\n", "    for i, ckpt_dir in enumerate(sorted(checkpoint_dirs)):\n", "        name = os.path.basename(ckpt_dir)\n", "        files = os.listdir(ckpt_dir)\n", "        checkpoint_files = [f for f in files if f.startswith('checkpoint_') and f.endswith('.pt')]\n", "        print(f'  [{i}] {name} ({len(checkpoint_files)} checkpoint files)')\n", "    \n", "    selected_checkpoint = sorted(checkpoint_dirs)[-1]\n", "    checkpoint_name = os.path.basename(selected_checkpoint)\n", "    \n", "    print(f'\n", "Resuming from: {checkpoint_name}')\n", "    print(f'Location: {selected_checkpoint}')\n", "    \n", "    DATASET = 'basketball'\n", "    MODEL_ARCH = 'rny002_gsm'\n", "    TEMPORAL_ARCH = 'gru'\n", "    BATCH_SIZE = 8\n", "    CLIP_LEN = 30\n", "    NUM_EPOCHS = 30\n", "    LEARNING_RATE = 0.0005\n", "    CROP_DIM = 224\n", "    \n", "    print('\n", "' + '='*80)\n", "    print('RESUMING TRAINING')\n", "    print('='*80)\n", "    print(f'Frames:   {TRAINING_FRAME_DIR}')\n", "    print(f'Checkpoint: {checkpoint_name}')\n", "    print(f'Save dir:   {selected_checkpoint}')\n", "    print('='*80)\n", "    print()\n", "    \n", "    start_time = time.time()\n", "    \n", "    !python3 train_e2e.py \"{DATASET}\" \"{TRAINING_FRAME_DIR}\"         -m \"{MODEL_ARCH}\"         -t \"{TEMPORAL_ARCH}\"         -s \"{selected_checkpoint}\"         --clip_len {CLIP_LEN}         --crop_dim {CROP_DIM}         --batch_size {BATCH_SIZE}         --num_epochs {NUM_EPOCHS}         --learning_rate {LEARNING_RATE}         --mixup ''         --criterion map         --dilate_len 2         --fg_upsample 0.7         --start_val_epoch 5         --warm_up_epochs 3         -j 0         --resume\n", "    \n", "    elapsed = time.time() - start_time\n", "    print('='*80)\n", "    print(f'\u2713 Training session completed in {elapsed/3600:.1f} hours!')\n", "    print('='*80)\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Cell 9: Check Training Progress\n", "\n", "View training metrics and progress without interrupting training.\n", "\n", "Run this anytime to check:\n", "- Which checkpoints have been saved\n", "- Training loss history\n", "- Validation mAP (if computed)\n", "- Current training status"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os, json\n", "from glob import glob\n", "\n", "checkpoint_dirs = sorted(glob(os.path.join(CHECKPOINT_DIR, 'basketball_*')))\n", "\n", "if not checkpoint_dirs:\n", "    print('No training runs found yet.')\n", "    print('Start training with Cell 7.')\n", "else:\n", "    latest_run = checkpoint_dirs[-1]\n", "    run_name = os.path.basename(latest_run)\n", "    \n", "    print(f'Latest training run: {run_name}')\n", "    print(f'Location: {latest_run}\n')\n", "    \n", "    checkpoint_files = sorted([f for f in os.listdir(latest_run) if f.startswith('checkpoint_') and f.endswith('.pt')])\n", "    if checkpoint_files:\n", "        print(f'Saved checkpoints ({len(checkpoint_files)}):')\n", "        for ckpt in checkpoint_files:\n", "            ckpt_path = os.path.join(latest_run, ckpt)\n", "            size_mb = os.path.getsize(ckpt_path)/(1024*1024)\n", "            print(f'  {ckpt} ({size_mb:.1f} MB)')\n", "    else:\n", "        print('No checkpoint files saved yet (training may be starting...)')\n", "    \n", "    loss_file = os.path.join(latest_run, 'loss.json')\n", "    if os.path.exists(loss_file):\n", "        print('\\nTraining history (last 10 epochs):')\n", "        try:\n", "            data = json.load(open(loss_file))\n", "            if isinstance(data, list):\n", "                for entry in data[-10:]:\n", "                    epoch = entry.get('epoch')\n", "                    train = entry.get('train')\n", "                    val = entry.get('val')\n", "                    m = entry.get('val_mAP', 'N/A')\n", "                    print(f'  Epoch {epoch:3d}: train={train:.4f} val={val:.4f} mAP={m}')\n", "            else:\n", "                print('  Unexpected loss.json format')\n", "        except Exception as e:\n", "            print(f'  Could not read loss.json: {e}')\n", "    else:\n", "        print('\\nNo loss.json found yet (training may be in first epoch)')\n", "    \n", "    print('\\nFull directory contents:')\n", "    !ls -lh \"{latest_run}\"\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["---\n", "\n", "## Troubleshooting\n", "\n", "### Session disconnected during training\n", "1. Start new Colab session\n", "2. Run Cells 1-3 (setup + mount Drive)\n", "3. Skip Cell 4 (frames already in Drive)\n", "4. Run Cell 5 (clone repo)\n", "5. Run Cell 8 (resume training)\n", "\n", "### Out of memory error\n", "- Edit Cell 7, change `BATCH_SIZE = 8` to `BATCH_SIZE = 6` or `BATCH_SIZE = 4`\n", "- Restart runtime and run Cell 7 again\n", "\n", "### Frames not downloading\n", "- Check AWS credentials in Cell 3\n", "- Verify S3 bucket access: `!aws s3 ls s3://nba-foul-dataset-oh/`\n", "- Check internet connection\n", "- Re-run Cell 4 (aws s3 sync will resume interrupted downloads)\n", "\n", "### GPU not detected\n", "- Runtime \u2192 Change runtime type \u2192 GPU\n", "- If still no GPU, try Runtime \u2192 Factory reset runtime\n", "\n", "### Training is slow\n", "- Check GPU usage in Cell 1 (should show V100 or A100 with Colab Pro)\n", "- T4 GPU is slower (~25-30 min/epoch)\n", "- V100/A100 is faster (~15-20 min/epoch)\n", "\n", "---\n", "\n", "## Expected Results\n", "\n", "**Success criteria:**\n", "- Overall mAP @ tolerance=2: **\u22650.60** (good), **\u22650.65** (excellent)\n", "- Training time: ~14-17 hours\n", "- Final test mAP computed automatically\n", "\n", "**After training completes:**\n", "- Checkpoints are in Drive: `/content/drive/MyDrive/nba_foul_training/checkpoints/`\n", "- Best model saved as `checkpoint_best.pt`\n", "- Training history in `loss.json`\n", "- Test predictions in `pred-test.*.json`\n", "\n", "You can download checkpoints from Drive to your computer or use them directly for inference."]}], "metadata": {"accelerator": "GPU", "colab": {"provenance": [], "gpuType": "T4"}, "kernelspec": {"display_name": "Python 3", "name": "python3"}, "language_info": {"name": "python"}}, "nbformat": 4, "nbformat_minor": 0}